{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac2eaa2f",
   "metadata": {},
   "source": [
    "Разобраться с моделькой перевода (без механизма внимания) как она устроена, запустить для перевода с русского на английский (при желании можно взять другие пары языков)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcaa6436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af687787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G:\\\\Мой диск\\\\Colab Notebooks\\\\NLP\\\\Lesson10'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fda7c290",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"rus.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3de49430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = w.lower().strip()\n",
    "    w = re.sub(r\"([?.!,])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Zа-яА-Я?.!,']+\", \" \", w)\n",
    "    w = w.strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fa9bf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:2]]  for l in lines[:num_examples]]\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "823a8de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "en, ru = create_dataset(path_to_file, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d86918d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bf5163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "396050dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "  # creating cleaned input, output pairs\n",
    "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fa29730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(451436, 451436)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en), len(ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7550146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 100000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "672d5523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000 80000 20000 20000\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ccca6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b481aaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "13 ----> мы\n",
      "88 ----> были\n",
      "616 ----> счастливы\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "17 ----> we\n",
      "79 ----> were\n",
      "131 ----> happy\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8de276ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 300\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46e7d850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 15]), TensorShape([64, 11]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eae823b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=False,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    \n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd5692e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "# print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29fdf920",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e621d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "decoder_sample_x, decoder_sample_h = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52951e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 7335])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_sample_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63761e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 1024])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_sample_h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ee22888",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f835b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_nmt_checkpoints'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5eeef257",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "            predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e2ee081",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.7285\n",
      "Epoch 1 Batch 100 Loss 2.0302\n",
      "Epoch 1 Batch 200 Loss 1.8823\n",
      "Epoch 1 Batch 300 Loss 1.6206\n",
      "Epoch 1 Batch 400 Loss 1.5212\n",
      "Epoch 1 Batch 500 Loss 1.3477\n",
      "Epoch 1 Batch 600 Loss 1.2831\n",
      "Epoch 1 Batch 700 Loss 1.2307\n",
      "Epoch 1 Batch 800 Loss 1.2361\n",
      "Epoch 1 Batch 900 Loss 1.1497\n",
      "Epoch 1 Batch 1000 Loss 1.1108\n",
      "Epoch 1 Batch 1100 Loss 1.0012\n",
      "Epoch 1 Batch 1200 Loss 1.1429\n",
      "Epoch 1 Loss 1.4017\n",
      "Time taken for 1 epoch 1307.419626712799 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.8550\n",
      "Epoch 2 Batch 100 Loss 0.8577\n",
      "Epoch 2 Batch 200 Loss 0.7914\n",
      "Epoch 2 Batch 300 Loss 0.7636\n",
      "Epoch 2 Batch 400 Loss 0.8178\n",
      "Epoch 2 Batch 500 Loss 0.8123\n",
      "Epoch 2 Batch 600 Loss 0.6648\n",
      "Epoch 2 Batch 700 Loss 0.6588\n",
      "Epoch 2 Batch 800 Loss 0.7486\n",
      "Epoch 2 Batch 900 Loss 0.7269\n",
      "Epoch 2 Batch 1000 Loss 0.6116\n",
      "Epoch 2 Batch 1100 Loss 0.5634\n",
      "Epoch 2 Batch 1200 Loss 0.6617\n",
      "Epoch 2 Loss 0.7133\n",
      "Time taken for 1 epoch 1317.9228885173798 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.4588\n",
      "Epoch 3 Batch 100 Loss 0.4522\n",
      "Epoch 3 Batch 200 Loss 0.4143\n",
      "Epoch 3 Batch 300 Loss 0.4559\n",
      "Epoch 3 Batch 400 Loss 0.4537\n",
      "Epoch 3 Batch 500 Loss 0.3977\n",
      "Epoch 3 Batch 600 Loss 0.3670\n",
      "Epoch 3 Batch 700 Loss 0.3678\n",
      "Epoch 3 Batch 800 Loss 0.3866\n",
      "Epoch 3 Batch 900 Loss 0.4148\n",
      "Epoch 3 Batch 1000 Loss 0.3769\n",
      "Epoch 3 Batch 1100 Loss 0.3357\n",
      "Epoch 3 Batch 1200 Loss 0.3198\n",
      "Epoch 3 Loss 0.3980\n",
      "Time taken for 1 epoch 1294.713766336441 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.2093\n",
      "Epoch 4 Batch 100 Loss 0.2171\n",
      "Epoch 4 Batch 200 Loss 0.2268\n",
      "Epoch 4 Batch 300 Loss 0.2462\n",
      "Epoch 4 Batch 400 Loss 0.2845\n",
      "Epoch 4 Batch 500 Loss 0.2527\n",
      "Epoch 4 Batch 600 Loss 0.2775\n",
      "Epoch 4 Batch 700 Loss 0.2834\n",
      "Epoch 4 Batch 800 Loss 0.2571\n",
      "Epoch 4 Batch 900 Loss 0.2584\n",
      "Epoch 4 Batch 1000 Loss 0.2529\n",
      "Epoch 4 Batch 1100 Loss 0.2364\n",
      "Epoch 4 Batch 1200 Loss 0.2573\n",
      "Epoch 4 Loss 0.2417\n",
      "Time taken for 1 epoch 1251.5821776390076 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.1265\n",
      "Epoch 5 Batch 100 Loss 0.1372\n",
      "Epoch 5 Batch 200 Loss 0.1841\n",
      "Epoch 5 Batch 300 Loss 0.1360\n",
      "Epoch 5 Batch 400 Loss 0.1692\n",
      "Epoch 5 Batch 500 Loss 0.1428\n",
      "Epoch 5 Batch 600 Loss 0.1442\n",
      "Epoch 5 Batch 700 Loss 0.2012\n",
      "Epoch 5 Batch 800 Loss 0.2030\n",
      "Epoch 5 Batch 900 Loss 0.1618\n",
      "Epoch 5 Batch 1000 Loss 0.1785\n",
      "Epoch 5 Batch 1100 Loss 0.1721\n",
      "Epoch 5 Batch 1200 Loss 0.1930\n",
      "Epoch 5 Loss 0.1660\n",
      "Time taken for 1 epoch 1402.397501707077 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.1384\n",
      "Epoch 6 Batch 100 Loss 0.1007\n",
      "Epoch 6 Batch 200 Loss 0.1129\n",
      "Epoch 6 Batch 300 Loss 0.0860\n",
      "Epoch 6 Batch 400 Loss 0.1021\n",
      "Epoch 6 Batch 500 Loss 0.1083\n",
      "Epoch 6 Batch 600 Loss 0.1096\n",
      "Epoch 6 Batch 700 Loss 0.1112\n",
      "Epoch 6 Batch 800 Loss 0.1802\n",
      "Epoch 6 Batch 900 Loss 0.1149\n",
      "Epoch 6 Batch 1000 Loss 0.1465\n",
      "Epoch 6 Batch 1100 Loss 0.1529\n",
      "Epoch 6 Batch 1200 Loss 0.1432\n",
      "Epoch 6 Loss 0.1265\n",
      "Time taken for 1 epoch 1365.8622708320618 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.1049\n",
      "Epoch 7 Batch 100 Loss 0.0973\n",
      "Epoch 7 Batch 200 Loss 0.1335\n",
      "Epoch 7 Batch 300 Loss 0.1136\n",
      "Epoch 7 Batch 400 Loss 0.0809\n",
      "Epoch 7 Batch 500 Loss 0.1175\n",
      "Epoch 7 Batch 600 Loss 0.1460\n",
      "Epoch 7 Batch 700 Loss 0.1473\n",
      "Epoch 7 Batch 800 Loss 0.1048\n",
      "Epoch 7 Batch 900 Loss 0.1218\n",
      "Epoch 7 Batch 1000 Loss 0.1795\n",
      "Epoch 7 Batch 1100 Loss 0.1211\n",
      "Epoch 7 Batch 1200 Loss 0.1162\n",
      "Epoch 7 Loss 0.1082\n",
      "Time taken for 1 epoch 1553.4340164661407 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0662\n",
      "Epoch 8 Batch 100 Loss 0.0667\n",
      "Epoch 8 Batch 200 Loss 0.0689\n",
      "Epoch 8 Batch 300 Loss 0.1002\n",
      "Epoch 8 Batch 400 Loss 0.0627\n",
      "Epoch 8 Batch 500 Loss 0.1032\n",
      "Epoch 8 Batch 600 Loss 0.0971\n",
      "Epoch 8 Batch 700 Loss 0.1124\n",
      "Epoch 8 Batch 800 Loss 0.0791\n",
      "Epoch 8 Batch 900 Loss 0.0774\n",
      "Epoch 8 Batch 1000 Loss 0.1117\n",
      "Epoch 8 Batch 1100 Loss 0.1406\n",
      "Epoch 8 Batch 1200 Loss 0.1317\n",
      "Epoch 8 Loss 0.0958\n",
      "Time taken for 1 epoch 1609.0467796325684 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0930\n",
      "Epoch 9 Batch 100 Loss 0.0540\n",
      "Epoch 9 Batch 200 Loss 0.0691\n",
      "Epoch 9 Batch 300 Loss 0.0851\n",
      "Epoch 9 Batch 400 Loss 0.0835\n",
      "Epoch 9 Batch 500 Loss 0.0479\n",
      "Epoch 9 Batch 600 Loss 0.0513\n",
      "Epoch 9 Batch 700 Loss 0.0815\n",
      "Epoch 9 Batch 800 Loss 0.0894\n",
      "Epoch 9 Batch 900 Loss 0.0700\n",
      "Epoch 9 Batch 1000 Loss 0.0992\n",
      "Epoch 9 Batch 1100 Loss 0.1320\n",
      "Epoch 9 Batch 1200 Loss 0.1326\n",
      "Epoch 9 Loss 0.0888\n",
      "Time taken for 1 epoch 1492.6205718517303 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0774\n",
      "Epoch 10 Batch 100 Loss 0.0525\n",
      "Epoch 10 Batch 200 Loss 0.0571\n",
      "Epoch 10 Batch 300 Loss 0.0548\n",
      "Epoch 10 Batch 400 Loss 0.0805\n",
      "Epoch 10 Batch 500 Loss 0.0736\n",
      "Epoch 10 Batch 600 Loss 0.0727\n",
      "Epoch 10 Batch 700 Loss 0.0790\n",
      "Epoch 10 Batch 800 Loss 0.1018\n",
      "Epoch 10 Batch 900 Loss 0.1316\n",
      "Epoch 10 Batch 1000 Loss 0.0878\n",
      "Epoch 10 Batch 1100 Loss 0.0785\n",
      "Epoch 10 Batch 1200 Loss 0.1009\n",
      "Epoch 10 Loss 0.0835\n",
      "Time taken for 1 epoch 1603.0393495559692 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.0855\n",
      "Epoch 11 Batch 100 Loss 0.0561\n",
      "Epoch 11 Batch 200 Loss 0.0653\n",
      "Epoch 11 Batch 300 Loss 0.0660\n",
      "Epoch 11 Batch 400 Loss 0.1124\n",
      "Epoch 11 Batch 500 Loss 0.0834\n",
      "Epoch 11 Batch 600 Loss 0.0807\n",
      "Epoch 11 Batch 700 Loss 0.0558\n",
      "Epoch 11 Batch 800 Loss 0.0742\n",
      "Epoch 11 Batch 900 Loss 0.0748\n",
      "Epoch 11 Batch 1000 Loss 0.0916\n",
      "Epoch 11 Batch 1100 Loss 0.0778\n",
      "Epoch 11 Batch 1200 Loss 0.0666\n",
      "Epoch 11 Loss 0.0815\n",
      "Time taken for 1 epoch 1684.1601366996765 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.0957\n",
      "Epoch 12 Batch 100 Loss 0.0804\n",
      "Epoch 12 Batch 200 Loss 0.0357\n",
      "Epoch 12 Batch 300 Loss 0.0677\n",
      "Epoch 12 Batch 400 Loss 0.0513\n",
      "Epoch 12 Batch 500 Loss 0.0994\n",
      "Epoch 12 Batch 600 Loss 0.0861\n",
      "Epoch 12 Batch 700 Loss 0.0886\n",
      "Epoch 12 Batch 800 Loss 0.0541\n",
      "Epoch 12 Batch 900 Loss 0.0914\n",
      "Epoch 12 Batch 1000 Loss 0.0914\n",
      "Epoch 12 Batch 1100 Loss 0.0867\n",
      "Epoch 12 Batch 1200 Loss 0.0905\n",
      "Epoch 12 Loss 0.0783\n",
      "Time taken for 1 epoch 1504.4041683673859 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.0669\n",
      "Epoch 13 Batch 100 Loss 0.0634\n",
      "Epoch 13 Batch 200 Loss 0.0635\n",
      "Epoch 13 Batch 300 Loss 0.0701\n",
      "Epoch 13 Batch 400 Loss 0.0699\n",
      "Epoch 13 Batch 500 Loss 0.0856\n",
      "Epoch 13 Batch 600 Loss 0.0690\n",
      "Epoch 13 Batch 700 Loss 0.0781\n",
      "Epoch 13 Batch 800 Loss 0.0648\n",
      "Epoch 13 Batch 900 Loss 0.0652\n",
      "Epoch 13 Batch 1000 Loss 0.0743\n",
      "Epoch 13 Batch 1100 Loss 0.0731\n",
      "Epoch 13 Batch 1200 Loss 0.0978\n",
      "Epoch 13 Loss 0.0766\n",
      "Time taken for 1 epoch 1652.1269271373749 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.0300\n",
      "Epoch 14 Batch 100 Loss 0.0911\n",
      "Epoch 14 Batch 200 Loss 0.0534\n",
      "Epoch 14 Batch 300 Loss 0.0506\n",
      "Epoch 14 Batch 400 Loss 0.0472\n",
      "Epoch 14 Batch 500 Loss 0.0908\n",
      "Epoch 14 Batch 600 Loss 0.0951\n",
      "Epoch 14 Batch 700 Loss 0.0344\n",
      "Epoch 14 Batch 800 Loss 0.0966\n",
      "Epoch 14 Batch 900 Loss 0.0749\n",
      "Epoch 14 Batch 1000 Loss 0.0608\n",
      "Epoch 14 Batch 1100 Loss 0.0828\n",
      "Epoch 14 Batch 1200 Loss 0.1331\n",
      "Epoch 14 Loss 0.0731\n",
      "Time taken for 1 epoch 1589.008630990982 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.0455\n",
      "Epoch 15 Batch 100 Loss 0.0324\n",
      "Epoch 15 Batch 200 Loss 0.0450\n",
      "Epoch 15 Batch 300 Loss 0.0610\n",
      "Epoch 15 Batch 400 Loss 0.0915\n",
      "Epoch 15 Batch 500 Loss 0.0471\n",
      "Epoch 15 Batch 600 Loss 0.0837\n",
      "Epoch 15 Batch 700 Loss 0.0737\n",
      "Epoch 15 Batch 800 Loss 0.0728\n",
      "Epoch 15 Batch 900 Loss 0.0672\n",
      "Epoch 15 Batch 1000 Loss 0.0759\n",
      "Epoch 15 Batch 1100 Loss 0.0740\n",
      "Epoch 15 Batch 1200 Loss 0.1065\n",
      "Epoch 15 Loss 0.0728\n",
      "Time taken for 1 epoch 1621.4494605064392 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.0627\n",
      "Epoch 16 Batch 100 Loss 0.0765\n",
      "Epoch 16 Batch 200 Loss 0.0648\n",
      "Epoch 16 Batch 300 Loss 0.0885\n",
      "Epoch 16 Batch 400 Loss 0.0631\n",
      "Epoch 16 Batch 500 Loss 0.0896\n",
      "Epoch 16 Batch 600 Loss 0.0452\n",
      "Epoch 16 Batch 700 Loss 0.0997\n",
      "Epoch 16 Batch 800 Loss 0.0957\n",
      "Epoch 16 Batch 900 Loss 0.0521\n",
      "Epoch 16 Batch 1000 Loss 0.0615\n",
      "Epoch 16 Batch 1100 Loss 0.0446\n",
      "Epoch 16 Batch 1200 Loss 0.0994\n",
      "Epoch 16 Loss 0.0706\n",
      "Time taken for 1 epoch 1554.6992404460907 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.0343\n",
      "Epoch 17 Batch 100 Loss 0.0661\n",
      "Epoch 17 Batch 200 Loss 0.0572\n",
      "Epoch 17 Batch 300 Loss 0.1126\n",
      "Epoch 17 Batch 400 Loss 0.0340\n",
      "Epoch 17 Batch 500 Loss 0.0576\n",
      "Epoch 17 Batch 600 Loss 0.0722\n",
      "Epoch 17 Batch 700 Loss 0.0495\n",
      "Epoch 17 Batch 800 Loss 0.0598\n",
      "Epoch 17 Batch 900 Loss 0.0840\n",
      "Epoch 17 Batch 1000 Loss 0.0852\n",
      "Epoch 17 Batch 1100 Loss 0.0479\n",
      "Epoch 17 Batch 1200 Loss 0.0551\n",
      "Epoch 17 Loss 0.0689\n",
      "Time taken for 1 epoch 1428.2763464450836 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.0694\n",
      "Epoch 18 Batch 100 Loss 0.0340\n",
      "Epoch 18 Batch 200 Loss 0.0578\n",
      "Epoch 18 Batch 300 Loss 0.0225\n",
      "Epoch 18 Batch 400 Loss 0.0698\n",
      "Epoch 18 Batch 500 Loss 0.0781\n",
      "Epoch 18 Batch 600 Loss 0.0597\n",
      "Epoch 18 Batch 700 Loss 0.0751\n",
      "Epoch 18 Batch 800 Loss 0.0712\n",
      "Epoch 18 Batch 900 Loss 0.0623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Batch 1000 Loss 0.1072\n",
      "Epoch 18 Batch 1100 Loss 0.0418\n",
      "Epoch 18 Batch 1200 Loss 0.0793\n",
      "Epoch 18 Loss 0.0680\n",
      "Time taken for 1 epoch 1566.7207083702087 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.0518\n",
      "Epoch 19 Batch 100 Loss 0.0379\n",
      "Epoch 19 Batch 200 Loss 0.0580\n",
      "Epoch 19 Batch 300 Loss 0.0827\n",
      "Epoch 19 Batch 400 Loss 0.0661\n",
      "Epoch 19 Batch 500 Loss 0.0660\n",
      "Epoch 19 Batch 600 Loss 0.0687\n",
      "Epoch 19 Batch 700 Loss 0.0714\n",
      "Epoch 19 Batch 800 Loss 0.0563\n",
      "Epoch 19 Batch 900 Loss 0.0627\n",
      "Epoch 19 Batch 1000 Loss 0.0650\n",
      "Epoch 19 Batch 1100 Loss 0.0704\n",
      "Epoch 19 Batch 1200 Loss 0.1471\n",
      "Epoch 19 Loss 0.0671\n",
      "Time taken for 1 epoch 1522.8802633285522 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.0589\n",
      "Epoch 20 Batch 100 Loss 0.0776\n",
      "Epoch 20 Batch 200 Loss 0.0482\n",
      "Epoch 20 Batch 300 Loss 0.0523\n",
      "Epoch 20 Batch 400 Loss 0.0680\n",
      "Epoch 20 Batch 500 Loss 0.0474\n",
      "Epoch 20 Batch 600 Loss 0.0565\n",
      "Epoch 20 Batch 700 Loss 0.0748\n",
      "Epoch 20 Batch 800 Loss 0.1100\n",
      "Epoch 20 Batch 900 Loss 0.0312\n",
      "Epoch 20 Batch 1000 Loss 0.0750\n",
      "Epoch 20 Batch 1100 Loss 0.1140\n",
      "Epoch 20 Batch 1200 Loss 0.0907\n",
      "Epoch 20 Loss 0.0657\n",
      "Time taken for 1 epoch 1260.9493126869202 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c779f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b51ff4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "\n",
    "    # storing the attention weights to plot later on\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "654ac845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence = evaluate(sentence)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14c14e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x24adf533910>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95d3839b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> здесь хорошо . <end>\n",
      "Predicted translation: it's good here . <end> \n"
     ]
    }
   ],
   "source": [
    "translate('Здесь хорошо.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dcf7e288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> я никогда такого не делаю . <end>\n",
      "Predicted translation: i never do that . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'Я никогда такого не делаю.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce171bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
